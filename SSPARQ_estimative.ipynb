{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8d40e58e-efc2-4947-b808-455c3a239acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from tqdm import tqdm\n",
    "from multiprocessing import Pool\n",
    "from sklearn import preprocessing\n",
    "from obspy.signal.headers import clibsignal\n",
    "import math\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.path as mpath\n",
    "from matplotlib import cm\n",
    "from mpl_toolkits.axes_grid1.inset_locator import inset_axes\n",
    "from matplotlib.ticker import MultipleLocator, FormatStrFormatter\n",
    "from matplotlib.dates import YearLocator, MonthLocator, DayLocator, HourLocator, MinuteLocator, SecondLocator, DateFormatter\n",
    "import matplotlib.dates as mdates\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "\n",
    "import obspy as op\n",
    "from obspy import read,read_inventory,read_events, UTCDateTime, Stream, Trace\n",
    "from obspy.clients.fdsn.client import Client\n",
    "from obspy.signal.rotate import rotate_ne_rt\n",
    "from obspy.geodetics import gps2dist_azimuth,kilometers2degrees\n",
    "from obspy.taup import TauPyModel\n",
    "\n",
    "import json\n",
    "import glob\n",
    "import os\n",
    "import numpy as np\n",
    "from itertools import combinations\n",
    "import pandas as pd\n",
    "\n",
    "import pyarrow.feather as feather\n",
    "\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2adcd26-b9a9-45b2-ac84-3b7fa5358563",
   "metadata": {},
   "source": [
    "____________\n",
    "# Setup\n",
    "____________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aefd64e5-3508-40bc-aa87-25629f904c50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading configuration file: ./config_file.cnf\n"
     ]
    }
   ],
   "source": [
    "from parameters_py.config import (\n",
    "\t\t\t\t\tWAVEFORM_DIR,CATALOG_FILE,XML_DIR,ORIENTATION_OUTPUT\n",
    "\t\t\t\t   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "75238516-000e-4942-b1f1-19f6ad6b61b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/sysop/dados_posdoc/PROJETO_RSBR_15_YEARS/OUTPUT/'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ORIENTATION_OUTPUT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd8131ee-42db-44ad-aedf-c11a8e44fe5d",
   "metadata": {},
   "source": [
    "---\n",
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f48373d9-8612-453e-8541-b524a99126fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.analyse import (\n",
    "\t\t\t\t\taic_simple,find_orientation,Braunmiller_Pornsopin_algorithm,calculate_metrics\n",
    "\t\t\t\t   )\n",
    "\n",
    "from src.utils import (\n",
    "\t\t\t\t\tquakeml_to_dataframe,moment_tensor_to_nodal_planes,calculate_plunge,mecclass,adjust_baz_for_ZEN,rms,energy\n",
    "\t\t\t\t   )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c48e567-c721-45f0-ae93-c26b63ecec53",
   "metadata": {},
   "source": [
    "---\n",
    "# Main program\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5816da9d-0a59-47f2-913b-920bfea570e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============\n",
      "Reading catalog\n",
      "===============\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/sysop/dados_posdoc/RSBR_15_years/obspyDMT/CATALOG/CMTSOLUTIONS_2010_2025.xml'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m===============\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 6\u001b[0m cat \u001b[38;5;241m=\u001b[39m quakeml_to_dataframe(CATALOG_FILE)\n\u001b[1;32m      7\u001b[0m cat\u001b[38;5;241m.\u001b[39mtail(\u001b[38;5;241m2\u001b[39m)\n",
      "File \u001b[0;32m~/diogogit/RSBR_15_years/src/utils.py:6\u001b[0m, in \u001b[0;36mquakeml_to_dataframe\u001b[0;34m(quakeml_file)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mquakeml_to_dataframe\u001b[39m(quakeml_file):\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;66;03m# Read the QuakeML file using ObsPy\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m     catalog \u001b[38;5;241m=\u001b[39m read_events(quakeml_file)\n\u001b[1;32m      8\u001b[0m     entries \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m event \u001b[38;5;129;01min\u001b[39;00m catalog:\n\u001b[1;32m     11\u001b[0m         \u001b[38;5;66;03m# Extract basic event information\u001b[39;00m\n",
      "File \u001b[0;32m~/Programs/anaconda3/lib/python3.11/site-packages/decorator.py:232\u001b[0m, in \u001b[0;36mdecorate.<locals>.fun\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    230\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kwsyntax:\n\u001b[1;32m    231\u001b[0m     args, kw \u001b[38;5;241m=\u001b[39m fix(args, kw, sig)\n\u001b[0;32m--> 232\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m caller(func, \u001b[38;5;241m*\u001b[39m(extras \u001b[38;5;241m+\u001b[39m args), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n",
      "File \u001b[0;32m~/Programs/anaconda3/lib/python3.11/site-packages/obspy/core/util/decorator.py:297\u001b[0m, in \u001b[0;36mmap_example_filename.<locals>._map_example_filename\u001b[0;34m(func, *args, **kwargs)\u001b[0m\n\u001b[1;32m    295\u001b[0m                 \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m:\n\u001b[1;32m    296\u001b[0m                     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m--> 297\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/Programs/anaconda3/lib/python3.11/site-packages/obspy/core/event/catalog.py:810\u001b[0m, in \u001b[0;36mread_events\u001b[0;34m(pathname_or_url, format, **kwargs)\u001b[0m\n\u001b[1;32m    808\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _create_example_catalog()\n\u001b[1;32m    809\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 810\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _generic_reader(pathname_or_url, _read, \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mformat\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/Programs/anaconda3/lib/python3.11/site-packages/obspy/core/util/base.py:656\u001b[0m, in \u001b[0;36m_generic_reader\u001b[0;34m(pathname_or_url, callback_func, **kwargs)\u001b[0m\n\u001b[1;32m    654\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo file matching file pattern: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m pathname)\n\u001b[1;32m    655\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m glob\u001b[38;5;241m.\u001b[39mhas_magic(pathname) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m Path(pathname)\u001b[38;5;241m.\u001b[39mis_file():\n\u001b[0;32m--> 656\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo such file or directory\u001b[39m\u001b[38;5;124m\"\u001b[39m, pathname)\n\u001b[1;32m    658\u001b[0m generic \u001b[38;5;241m=\u001b[39m callback_func(pathnames[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    659\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(pathnames) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/sysop/dados_posdoc/RSBR_15_years/obspyDMT/CATALOG/CMTSOLUTIONS_2010_2025.xml'"
     ]
    }
   ],
   "source": [
    "print('===============')\n",
    "print('Reading catalog')\n",
    "print('===============')\n",
    "print('\\n')\n",
    "\n",
    "cat = quakeml_to_dataframe(CATALOG_FILE)\n",
    "cat.tail(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f8d55cd-c32e-48dc-a5fa-d249eb0d8583",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('================')\n",
    "print('Reading stations')\n",
    "print('================')\n",
    "print('\\n')\n",
    "\n",
    "STATIONS_xml = sorted(glob.glob(XML_DIR+'*BR.CZ*'))\n",
    "print('Number of stations:',len(STATIONS_xml))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a4eb8bf-7618-4643-987f-deb4973dffa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "STATIONS_xml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c7b533b-87e8-4340-8c5b-0e559f84ea2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_orientation(input_lst):\n",
    "\n",
    "    xml_file = input_lst[0]\n",
    "    wave_dir = input_lst[1]\n",
    "    \n",
    "    # ---------------\n",
    "    # Read XML file\n",
    "                        \n",
    "    station_xml = op.read_inventory(xml_file)\n",
    "    network = station_xml[0].code\n",
    "    station = station_xml[0][0].code    \n",
    "    \n",
    "    # ---------------------------\n",
    "    # Retrieving events waveforms\n",
    "    # ---------------------------\n",
    "    \n",
    "    for evid,event in tqdm(cat.iterrows(), total=len(cat),desc=station+' orientation'):\n",
    "        # ------------------------------\n",
    "        # Check if the event is eligible\n",
    "\n",
    "        evdp = event['depth']\n",
    "        evmag = event['mag']\n",
    "        evtype = event['magType']\n",
    "        evtime = UTCDateTime(event['time'])\n",
    "        evname = UTCDateTime(event['time']).strftime('%Y.%j.%H.%M.%S')\n",
    "        evname2 = UTCDateTime(event['time']).strftime('%Y%m%d_%H%M%S')\n",
    "\n",
    "        year = UTCDateTime(event['time']).strftime('%Y')\n",
    "        julian_day = UTCDateTime(event['time']).strftime('%j')\n",
    "                            \n",
    "        # Epicentral distance:\n",
    "            \n",
    "        stlo = station_xml[-1][-1][-1].longitude\n",
    "        stla = station_xml[-1][-1][-1].latitude\n",
    "            \n",
    "        evla = event['latitude']\n",
    "        evlo = event['longitude']\n",
    "            \n",
    "        dist,az,baz = gps2dist_azimuth(evla, evlo,stla, stlo)\n",
    "        gcarc = kilometers2degrees(dist/1000)\n",
    "        \n",
    "        # Taup: theoretical travel times \n",
    "        model = TauPyModel(model=\"iasp91\")\n",
    "        arrivals = model.get_travel_times(source_depth_in_km=evdp,distance_in_degree=gcarc,phase_list=['P','PKP','PKIKP'])\n",
    "        \n",
    "        if len(arrivals) > 0 and (gcarc < 100 or 140 < gcarc <= 180):\n",
    "            \n",
    "            # Event time + phase arrival time\n",
    "            evtime = evtime+arrivals[0].time\n",
    "            \n",
    "            # ----------------------------\n",
    "            # Check if feather file exists\n",
    "            \n",
    "            output_FEATHER_FILES_ORIENTATION = ORIENTATION_OUTPUT+'FEATHER_FILES/ORIENTATION/'+network+'.'+station+'/'\n",
    "            \n",
    "            file_feather_name = output_FEATHER_FILES_ORIENTATION+network+'.'+station+'.'+evname+'.orientation.feather'\n",
    "    \n",
    "            station_pwd = glob.glob(wave_dir + '/*')\n",
    "   \n",
    "            if os.path.isfile(file_feather_name):\n",
    "                pass\n",
    "        \n",
    "            else:\n",
    "                # -------------------------------\n",
    "                # Check if components file exists\n",
    "                        \n",
    "                if 1 == 1:    \n",
    "\n",
    "                    files = [x for x in station_pwd if x.endswith(evname2)]\n",
    "\n",
    "                    if len(files) >= 3:\n",
    "                        try:\n",
    "                            file_HHE = files[component_index(files, \"E\")]\n",
    "                            file_HHN = files[component_index(files, \"N\")]\n",
    "                            file_HHZ = files[component_index(files, \"Z\")]\n",
    "\n",
    "                        except:\n",
    "\n",
    "                            file_HHE = files[component_index(files, \"2\")]\n",
    "                            file_HHN = files[component_index(files, \"1\")]\n",
    "                            file_HHZ = files[component_index(files, \"Z\")]\n",
    "\n",
    "            \n",
    "                        # # # --------\n",
    "                        # # # Data HHE\n",
    "                                \n",
    "                        tr2_data_file = op.read(file_HHE)\n",
    "                        tr2_data_file.trim(evtime-TIME_WINDOW,evtime+TIME_WINDOW)\n",
    "                        tr2_data_file.taper(type='cosine',max_percentage=0.1)\n",
    "                        tr2_data_file.filter('bandpass',freqmin=PERIOD_BANDS[0],freqmax=PERIOD_BANDS[1],zerophase=True, corners=4)\n",
    "                    \n",
    "                        # --------\n",
    "                        # Data HHN\n",
    "                    \n",
    "                        tr1_data_file = op.read(file_HHN)\n",
    "                        tr1_data_file.trim(evtime-TIME_WINDOW,evtime+TIME_WINDOW)\n",
    "                        tr1_data_file.taper(type='cosine',max_percentage=0.1)\n",
    "                        tr1_data_file.filter('bandpass',freqmin=PERIOD_BANDS[0],freqmax=PERIOD_BANDS[1],zerophase=True, corners=4)\n",
    "                            \n",
    "                        # --------\n",
    "                        # Data HHZ\n",
    "                                \n",
    "                        trZ_data_file = op.read(file_HHZ)\n",
    "                        trZ_data_file.trim(evtime-TIME_WINDOW,evtime+TIME_WINDOW)\n",
    "                        trZ_data_file.taper(type='cosine',max_percentage=0.1)\n",
    "                        trZ_data_file.filter('bandpass',freqmin=PERIOD_BANDS[0],freqmax=PERIOD_BANDS[1],zerophase=True, corners=4)\n",
    "\n",
    "                        if len(tr2_data_file) > 0 and len(tr1_data_file) > 0 and len(trZ_data_file) > 0:\n",
    "                               \n",
    "                             if (tr2_data_file[0].stats.npts == tr1_data_file[0].stats.npts == trZ_data_file[0].stats.npts) and (trZ_data_file[0].stats.npts > TIME_WINDOW * 40):\n",
    "                                \n",
    "                                tr2_data_filtered = tr2_data_file[0].data[1000:-1000]\n",
    "            \n",
    "                                tr1_data_filtered = tr1_data_file[0].data[1000:-1000]\n",
    "                                    \n",
    "                                trZ_data_filtered = trZ_data_file[0].data[1000:-1000]\n",
    "                                trZ_time = trZ_data_file[0].times()[1000:-1000]-TIME_WINDOW\n",
    "\n",
    "                                # -------------------------------------------\n",
    "                                # Function estimates the Akaike Information directly from data \n",
    "                                # The Summed Log Likelihood section implies that a natural \n",
    "                                # changepoint estimate is the sample index that minimizes \n",
    "                                # the AIC in equation\n",
    "                                \n",
    "                                aic_curve = aic_simple(trZ_data_filtered)\n",
    "                                \n",
    "                                k_min_index = np.argmin(aic_curve)\n",
    "                            \n",
    "                                time_P_arr = trZ_time[k_min_index]\n",
    "                                                        \n",
    "                                # Time error:\n",
    "                                time_ins = round(time_P_arr,1)\n",
    "                            \n",
    "                                if time_ins > 0:\n",
    "                                    signal_window_start = time_ins\n",
    "                                    signal_window_final = time_ins+TIME_FINAL_P\n",
    "                                    noise_window_start = time_ins\n",
    "                                    noise_window_final = time_ins-TIME_FINAL_P\n",
    "                                else:\n",
    "                                    signal_window_start = time_ins\n",
    "                                    signal_window_final = TIME_FINAL_P+time_ins\n",
    "                                    noise_window_start = time_ins\n",
    "                                    noise_window_final = -(abs(time_ins)+TIME_FINAL_P) \n",
    "                                # -------------------------------------------------------------------------------------------------------------------------------\n",
    "                                # Signal and noise windows\n",
    "                                        \n",
    "                                signal_window = (trZ_time >= signal_window_start) & (trZ_time <= signal_window_final)\n",
    "                                noise_window = (trZ_time >= noise_window_final) & (trZ_time <= noise_window_start)\n",
    "                                \n",
    "                                noise = trZ_data_filtered[noise_window]\n",
    "                                trZ_noise_time = trZ_time[noise_window]\n",
    "            \n",
    "                                tr2 = tr2_data_filtered[signal_window]\n",
    "                                tr1 = tr1_data_filtered[signal_window]\n",
    "                                trZ = trZ_data_filtered[signal_window]\n",
    "                                trZ_signal_time = trZ_time[signal_window]\n",
    "                                    \n",
    "                                # -------------------------------------------------------------------------------------------------------------------------------\n",
    "                                # Calculating the optimal orientation\n",
    "                                                    \n",
    "                                results = Braunmiller_Pornsopin_algorithm(tr1,tr2,trZ,noise,baz,time_ins,CCVR_MIN=0.45,SNR_MIN=10,TRR_MIN=0.45,RVR_MIN=-1)\n",
    "\n",
    "                                # -------------------------------------------------------------------------------------------------------------------------------\n",
    "                                # Calculating the Plunge of: P, B, and T axis\n",
    "\n",
    "                                nodal_planes = moment_tensor_to_nodal_planes(event['moment tensor'])\n",
    "\n",
    "                                event_class = mecclass(nodal_planes)\n",
    "\n",
    "                                # ----------------------------------------------------------------------------------------------------                               \n",
    "                                # Creating a Pandas DataFrame:\n",
    "                                column_info = [network,station,stla,stlo,evname,evla,evlo,evtime,evmag,evtype,evdp,dist,gcarc,baz,tr1_data_filtered,tr2_data_filtered,trZ_data_filtered,trZ_time,results['SS_best'],results['signal_strength'],results['SZR_best'],results['similarity_ZR'],results['ERTR_best'],results['energy_ratio_TR'],results['ERRZ_best'],results['energy_ratio_RZ'],results['SNR'],results['phi'],results['theta'],aic_curve,time_ins,results['quality'],results['gain_HHN'],results['gain_HHE'],results['gain_HHZ'],event['moment tensor'],nodal_planes,event_class]\n",
    "                                columns_header = ['network','station','stla','stlo','evname','evla','evlo','evtime','evmag','evtype','evdp','distance','gcarc','baz','tr1_data','tr2_data','trZ_data','trZ_time','SS_best','signal_strength','SZR_best','similarity_vertical_radial','ERTR_best','energy_transverse_radial','ERRZ_best','energy_radial_vertical','SNR','phi','theta','aic_curve','clock_error','quality','gain_HHN','gain_HHE','gain_HHZ','moment tensor','nodal_planes','event_class']\n",
    "                                orient_p_wave_df = pd.DataFrame(column_info, index=columns_header).T\n",
    "                                orient_p_wave_df['evtime'] = pd.to_datetime(orient_p_wave_df['evtime'].apply(lambda x: x.isoformat() if isinstance(x, UTCDateTime) else x))\n",
    "\n",
    "                                # ----------------------------------------------------------------------------------------------------\n",
    "                                # Convert from pandas to Arrow and saving in feather formart file\n",
    "                                os.makedirs(output_FEATHER_FILES_ORIENTATION,exist_ok=True)\n",
    "                                feather.write_feather(orient_p_wave_df, file_feather_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28368e53-7a2d-4acd-a368-aef64c5149ed",
   "metadata": {},
   "source": [
    "# Creating input list for different folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c632f4-5e97-4fcd-b2e8-4cf0a7c39893",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_list = []\n",
    "for xml_f in STATIONS_xml:\n",
    "    for wa_dir in WAVEFORM_DIR:\n",
    "        input_list.append([xml_f,wa_dir])   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e65bd7c8-bc78-4c49-a91c-4284c239e6c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7aaff42-f461-40c1-90ff-78c6b83e6f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "with Pool(processes=num_processes) as p:\n",
    "    max_ = len(STATIONS_xml)\n",
    "    with tqdm(total=max_,) as pbar:\n",
    "        for result in p.imap_unordered(calculate_orientation,input_list):\n",
    "            pbar.update()\n",
    "\n",
    "print('\\n')\n",
    "print(\"--- %.2f execution time (min) ---\" % ((time.time() - start_time)/60))\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac9a13f6-a6e7-45bb-99a6-904caa2df3eb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
